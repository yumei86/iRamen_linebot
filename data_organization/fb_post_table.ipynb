{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import random\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from opencc import OpenCC\n",
    "import os, itertools\n",
    "from itertools import groupby\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizing Facebook Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'fb_crawling/data/fb_crawling_output.csv' does not exist: b'fb_crawling/data/fb_crawling_output.csv'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-90881abdc814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### load data from Fb_crawling_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fb_crawling/data/fb_crawling_output.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'fb_crawling/data/fb_crawling_output.csv' does not exist: b'fb_crawling/data/fb_crawling_output.csv'"
     ]
    }
   ],
   "source": [
    "### load data from Fb_crawling_output\n",
    "df = read_csv('fb_crawling/data/fb_crawling_output.csv', encoding='utf8',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove non-\"Â∫óÂÆ∂\" post ###\n",
    "temp_p_lst = []\n",
    "temp_d_lst = []\n",
    "temp_p = ''\n",
    "temp_d = ''\n",
    "\n",
    "for post_content, date_time in zip(df['Description'].astype(str), df['published'].astype(str)):\n",
    "    #cc = OpenCC('s2tw') # Á∞°ËΩâÁπÅ\n",
    "    #post_content = cc.convert(post_content)\n",
    "    post = post_content.replace(' ','').replace('ÔøºÔøº','').replace('ÔøºÔøºÔøº','').replace('\\n','') \\\n",
    "        .replace('$','').replace('Ôºè','').replace('‚Ä¶','').replace('Ôºö',':').replace(':',':').replace('%','') \\\n",
    "        .replace(u'\\u3000',u'').replace('‚ñé','').replace('Ôºâ',')').replace('Ôºà','(').replace('O','') \\\n",
    "        .replace('„Äê',' ').replace('„Äë',':').replace('‚ñ†','').replace('‚óÜ','').replace('‚óè','').replace('‚òÖ','') \\\n",
    "        .replace('1.','').replace('2.','').replace('3.','').replace('4.','').replace('5.','') \\\n",
    "        .replace('6.','').replace('7.','') \\\n",
    "        .replace('\\'','').replace('_','').replace('‚òÜ','') \\\n",
    "        .replace('‚ñé','').replace('üÅ¢',' ').replace('-','').replace('‚óé','')\\\n",
    "        .replace('Êõ¥Â§ö','').replace('ÂàÜÈöîÁ∑ö','').replace('NT','') \\\n",
    "        .replace('‚ñéÂ∫óÂÆ∂:','Â∫óÂÆ∂:').replace('‚ñéÂ∫ó„ÄÄ„ÄÄÂêç:','Â∫óÂÆ∂:') \\\n",
    "        .replace('Â∫óÂÆ∂/ÈÑ∞Ëøë','Â∫óÂÆ∂:').replace('Â∫óÂÆ∂/ÈÑ∞ËøëÂú∞Ê®ô:','Â∫óÂÆ∂:').replace('Â∫óÂÆ∂ÈÑ∞ËøëÂú∞Èªû:','Â∫óÂÆ∂:') \\\n",
    "        .replace('Â∫óÂÆ∂/ÈÑ∞ËøëÂú∞Èªû:','Â∫óÂÆ∂:') \\\n",
    "        .replace('Â∫óÂêç:','Â∫óÂÆ∂:').replace('‚ñéÂ∫ó„ÄÄ„ÄÄÂÆ∂:','Â∫óÂÆ∂:').replace(' Â∫óÂÆ∂','Â∫óÂÆ∂:')\\\n",
    "        .replace('„ÄêÂ∫óÂÆ∂„Äë','Â∫óÂÆ∂:').replace('Â∫óÂÆ∂ÂêçÁ®±:','Â∫óÂÆ∂:').replace('Â∫óÂÆ∂Ë≥áË®ä:','Â∫óÂÆ∂:') \\\n",
    "        .replace('ÈÑ∞Ëøë:','%ÈÑ∞ËøëÂú∞Èªû:').replace('ÊâÄÂú®Âú∞ÂçÄ','%ÈÑ∞ËøëÂú∞Èªû:') \\\n",
    "        .replace('ÈÑ∞Ëøë‰ΩçÁΩÆ:','%ÈÑ∞ËøëÂú∞Èªû:').replace('Âú∞Èªû:','%ÈÑ∞ËøëÂú∞Èªû:') \\\n",
    "        .replace('ÊâÄÂú®Âú∞ÂçÄ/ÈÑ∞Ëøë‰ΩçÁΩÆ:','%ÈÑ∞ËøëÂú∞Èªû:') \\\n",
    "        .replace('ÈÑ∞ËøëÂú∞Èªû','%ÈÑ∞ËøëÂú∞Èªû').replace('Ëá®ËøëÂú∞Èªû:','%ÈÑ∞ËøëÂú∞Èªû:').replace('Âú∞ÂùÄ:','%ÈÑ∞ËøëÂú∞Èªû:') \\\n",
    "        .replace('ÈÑ∞ËøëÂú∞ÂçÄ','%ÈÑ∞ËøëÂú∞Èªû').replace('ÂçÄÂüü','%ÈÑ∞ËøëÂú∞Èªû:').replace('Âú∞ÂçÄ/ÈÑ∞Ëøë','%ÈÑ∞ËøëÂú∞Èªû:')\\\n",
    "        .replace('‰ΩçÁΩÆ:','%ÈÑ∞ËøëÂú∞Èªû:').replace('‰ΩçÁΩÆ','%ÈÑ∞ËøëÂú∞Èªû:')\\\n",
    "        .replace('ÊãâÈ∫µÂêçÁ®±','%.GÊãâÈ∫µÂêçÁ®±').replace('È§êÈªûÂêçÁ®±:','%.GÊãâÈ∫µÂêçÁ®±:')\\\n",
    "        .replace('È§êÈªû:','%.GÊãâÈ∫µÂêçÁ®±:').replace('ÊãâÈ∫µÂìÅÈ†Ö:','%.GÊãâÈ∫µÂêçÁ®±')\\\n",
    "        .replace('ÂìÅÈ†Ö:','%.GÊãâÈ∫µÂêçÁ®±:').replace('ÂìÅÂêç:','%.GÊãâÈ∫µÂêçÁ®±:')\\\n",
    "        .replace('ÂêçÁ®±:','%.GÊãâÈ∫µÂêçÁ®±:').replace('ÂìÅÈ†ÖÂÉπÊ†º:','%.GÊãâÈ∫µÂêçÁ®±:')\\\n",
    "        .replace('ÈÖçÁΩÆ:','ZÈÖçÁΩÆ').replace('ÈÖç„ÄÄ„ÄÄÁΩÆ','ZÈÖçÁΩÆ').replace('ÈÖçÁΩÆ(','ZÈÖçÁΩÆ').replace('‚ñéÈÖçÁΩÆ:','ZÈÖçÁΩÆ')\\\n",
    "        .replace('ÂøÉÂæóÊÑüÊÉ≥:','Z').replace('ÊÑüÊÉ≥:','Z').replace('ÂøÉÂæó:','Z') \\\n",
    "        .replace('ÂÆåÈ£üÊÑüÂèó:','Z').replace('ÂÆåÈ£üÊÑüÊÉ≥:','Z') \\\n",
    "        .replace('Âë≥Â¢û','Âë≥Âôå').replace('È£üÂÖ´Áï™','È£ü8Áï™').replace('‰∫î„Éé','‰∫î‰πã').replace('Á∑è','Á∏Ω')\n",
    "    \n",
    "    if \"Â∫óÂÆ∂\" in post[:5]:\n",
    "        temp_p_lst.append(post)\n",
    "\n",
    "        for word in date_time:\n",
    "            temp_d += word\n",
    "            if word == 'Êó•':\n",
    "                break\n",
    "        if ('Â∞èÊôÇ' in temp_d) or ('ÂàÜÈêò' in temp_d):\n",
    "            temp_d = '2020Âπ¥12Êúà2Êó•'   # Change the current date as nedded\n",
    "        elif ('Êò®Â§©' in temp_d):\n",
    "            temp_d = '2020Âπ¥12Êúà1Êó•'   # Change the date as nedded\n",
    "        elif 'Âπ¥' not in temp_d:\n",
    "            temp_d = '2020Âπ¥' + temp_d      \n",
    "        temp_d_lst.append(temp_d)\n",
    "        temp_d = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Classifiy posts according to the standard format\n",
    "unorganized_shops = []\n",
    "unorganized_date = []\n",
    "ramen_shop_raw = []\n",
    "ramen_name_raw = []\n",
    "ramen_review_raw = []\n",
    "ramen_date_raw = []\n",
    "\n",
    "for shops, date in zip(temp_p_lst, temp_d_lst):\n",
    "    if ('%'not in shops or 'G' not in shops or 'Z' not in shops \\\n",
    "        or shops.index('Z')>shops.index('G')+80) :\n",
    "        unorganized_shops.append(shops)\n",
    "        unorganized_date.append(date)      \n",
    "    else:\n",
    "        ramen_shop_raw.append(shops[:shops.index('%')])\n",
    "        ramen_name_raw.append(shops[shops.index('G')+1:shops.index('Z')])\n",
    "        ramen_review_raw.append(shops[shops.index('Z')+1:shops.index('Z')+265]+'...')\n",
    "        ramen_date_raw.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### second filtering\n",
    "unorganized_unorganized_shops = []\n",
    "unorganized_unorganized_date = []\n",
    "\n",
    "for shops, date in zip(unorganized_shops, unorganized_date):\n",
    "    if ('G' in shops and '0' in shops):\n",
    "        ramen_shop_raw.append(shops[:shops.index('%')])\n",
    "        ramen_name_raw.append(shops[shops.index('G')+1 : shops.index('G')+35])\n",
    "        ramen_review_raw.append(shops[shops.index('G')+15 : shops.index('G')+285]+'...')\n",
    "        ramen_date_raw.append(date)\n",
    "    else:\n",
    "        unorganized_unorganized_shops.append(shops)\n",
    "        unorganized_unorganized_date.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dealing with shop_name\n",
    "ramen_shop_list = []\n",
    "\n",
    "for shops in ramen_shop_raw:\n",
    "    shops.replace('Â∫óÂÆ∂','').replace('#','').replace('ÈÑ∞Ëøë','')\n",
    "    if ':' in shops:\n",
    "        shops = shops[shops.index(':')+1:]\n",
    "    shops = emoji.demojize(shops)\n",
    "    shops = shops.replace('Ôºè','')\n",
    "    shops = re.sub(':\\S+?:', ' ', shops)\n",
    "    if ':' in shops:\n",
    "        shops = shops[:shops.index(':')]\n",
    "    if len(shops) <= 1:\n",
    "        shops = ''\n",
    "    if 'Âú∞ÂùÄ' in shops and 'Áî®È§ê' in shops:\n",
    "        if shops.index('Âú∞') < shops.index('Áî®'):\n",
    "            ramen_shop_list.append(shops[:shops.index('Âú∞')])\n",
    "        else:\n",
    "            ramen_shop_list.append(shops[:shops.index('Áî®')])\n",
    "    elif 'Âú∞ÂùÄ' in shops:\n",
    "        ramen_shop_list.append(shops[:shops.index('Âú∞')])\n",
    "    elif 'Áî®È§ê' in shops:\n",
    "        ramen_shop_list.append(shops[:shops.index('Áî®')])\n",
    "    else:\n",
    "        ramen_shop_list.append(shops)\n",
    "        \n",
    "### Double check shop_name\n",
    "ramen_shop_list_final = []\n",
    "for shop in ramen_shop_list:\n",
    "    shop = re.sub(r'[^\\w\\s]','',shop)\n",
    "    shop = shop.replace('#','').replace('ÈÑ∞Ëøë','').replace('Ëá®Ëøë','').replace('ÊéíÈöäÁãÄÊ≥Å','') \\\n",
    "    .replace('Êó•Êúü','').replace('Â∫óÂÆ∂','').replace('ÈôÑËøë','').replace('„Ää','').replace('„Äã','') \\\n",
    "    .replace('„ÄÇ','').replace('„ÄÅ',' ').replace('ÔΩú','').replace('Ôºü','').replace('Âú∞ÂçÄ','') \\\n",
    "    .replace('(','').replace(')','').replace('¬∑','').replace('/','').replace('Âú∞Ê®ô','') \\\n",
    "    .replace('‚Äª','').replace('„ÅÅ','„ÅÇ').replace('¬≤','2').replace(' ','')\n",
    "    if len(shop) >= 31:\n",
    "        shop = shop[:31]\n",
    "    if 'Êç∑ÈÅã' in shop:\n",
    "        shop = shop[:shop.index('Êç∑ÈÅã')]\n",
    "    ramen_shop_list_final.append(shop)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dealing with ramen_name \n",
    "ramen_name_list = []\n",
    "ramen_name_list_final = []\n",
    "\n",
    "for names in ramen_name_raw:\n",
    "    names = emoji.demojize(names)\n",
    "    names = re.sub(':\\S+?:', ' ', names)\n",
    "    new_name = names.replace('ÊãâÈ∫µ%.G','').replace('%.G','').replace('#','').replace('ÈÑ∞Ëøë','')\n",
    "    last_ch = new_name[-1]\n",
    "    first_ch = new_name[0]\n",
    "    # ramen_name_list = []\n",
    "    if ('0' in new_name) and ('00' not in new_name) and ('2020' not in new_name) \\\n",
    "        and (last_ch != ')'):\n",
    "        if new_name[-1] == 'ÂÖÉ' or new_name[-2:] == 'Êó•ÂÖÉ':\n",
    "            ramen_name_list.append(new_name[:new_name.index('ÂÖÉ')+1])\n",
    "        elif new_name[-2:] == 'Êó•Âúì':\n",
    "            ramen_name_list.append(new_name[:new_name.index('Âúì')+1])\n",
    "        else:\n",
    "            ramen_name_list.append(new_name[:new_name.index('0')+1])\n",
    "    elif '00' in new_name and last_ch != ')' and '2020' not in new_name:\n",
    "        ramen_name_list.append(new_name[:new_name.index('00')+2])\n",
    "    elif first_ch != 'Êãâ' and 'Êãâ' in new_name:\n",
    "        ramen_name_list.append(new_name[new_name.index('Êãâ'):])\n",
    "    elif '0' not in new_name and '00' not in new_name and '/' not in new_name\\\n",
    "        and last_ch.isdigit() == False and last_ch != 'È∫µ' \\\n",
    "        and last_ch != ')'and '+' not in new_name:\n",
    "        new_point =  new_name.replace('È∫µ','È∫µH').replace('ÊãâÈ∫µHÂêçÁ®±','ÊãâÈ∫µÂêçÁ®±')\n",
    "        if 'H' in new_point:\n",
    "            ramen_name_list.append(new_point[:new_point.index('H')])\n",
    "        else:\n",
    "            ramen_name_list.append(new_point)\n",
    "    else:\n",
    "        ramen_name_list.append(new_name)\n",
    "        \n",
    "### Double check ramen_name\n",
    "for name in ramen_name_list:\n",
    "    if ':' in name:\n",
    "        name = name[name.index(':')+1:]\n",
    "    name = name.replace('ÊãâÈ∫µÂêçÁ®±/ÂÉπÊ†º','').replace('ÊãâÈ∫µÂêçÁ®±','').replace('ÊãâÈ∫µÂêçÁ®±ÂÉπÊ†º','').replace('ÂÉπÊ†º','')\n",
    "    ramen_name_list_final.append(name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dealing with fb_review\n",
    "ramen_review_list = []\n",
    "ramen_review_list_final = []\n",
    "\n",
    "for name, reviews in zip(ramen_name_list, ramen_review_raw):\n",
    "    new_reviews = reviews.replace('ÊãâÈ∫µ%.G','').replace('%.G','').replace('%','').replace('$','')\n",
    "    first_few_words = new_reviews[0:6]\n",
    "    if 'ÈÖçÁΩÆ'not in first_few_words:\n",
    "        for i in range(-(len(name)),0,1):\n",
    "            if (name[i:]) == (new_reviews[:-i]):\n",
    "                updated_review = new_reviews[-i:]\n",
    "                ramen_review_list.append(updated_review)\n",
    "                break\n",
    "            elif (i == -1) and (ramen_name_list[i:]) != (ramen_review_raw[:-i]):\n",
    "                pattern=\"[\\u4e00-\\u9fa5]+\" \n",
    "                regex = re.compile(pattern)\n",
    "                results =  regex.findall(new_reviews)\n",
    "                results_to_str =' '.join([str(elem) for elem in results]) \n",
    "                ramen_review_list.append(results_to_str)\n",
    "    else:\n",
    "        ramen_review_list.append(new_reviews)\n",
    "\n",
    "### Double check fb_review\n",
    "punc = [',' , '.' , 'Ôºå', '„ÄÇ', ')',' ','‚Ä¶‚Ä¶','Ôºõ','„ÄÅ','\\'','/','?','ÂÖÉ']\n",
    "for reviews in ramen_review_list:\n",
    "    if reviews[0] in punc:\n",
    "        reviews = reviews[1:]\n",
    "    ramen_review_list_final.append(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove emoji in fb_review\n",
    "review_lst = []\n",
    "def give_emoji_free_text(text):\n",
    "    allchars = [str for str in text]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ''.join([str for str in text if not any(i in str for i in emoji_list)])\n",
    "    return clean_text\n",
    "\n",
    "for r in ramen_review_list_final:\n",
    "    r = give_emoji_free_text(r)\n",
    "    review_lst.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create stem_store_name\n",
    "stem_store = []\n",
    "\n",
    "for store in ramen_shop_list_final:\n",
    "    store = store.lower()\n",
    "    store = re.sub(r'[^\\w\\s]','',store)\n",
    "    store = store.replace('Á∑è','Á∏Ω').replace('È∫∫','È∫µ').replace('ÈÜ§','ÈÜ¨').replace('„Çâ„Éº„ÇÅ„Çì‰∏ÉÈù¢','‰∏ÉÈù¢') \\\n",
    "                .replace('Ëá∫ÂçóÁôΩÈú≤','ÁôΩÈú≤').replace('È∑πÊµÅÊù±‰∫¨Ë±öÈ™®ÊãâÈ∫µÊ•µÂå†','È∑πÊµÅÊ•µÂå†') \\\n",
    "                .replace('Â§™ÈôΩËïÉËåÑEXPRESS','Â§™ÈôΩËïÉËåÑÊãâÈ∫µEXPRESS') \\\n",
    "                .replace('È∑πÊµÅÊù±‰∫¨ÈÜ¨Ê≤πÊãâÈ∫µËò≠‰∏∏','È∑πÊµÅËò≠‰∏∏').replace('È∑πÊµÅËá∫ÁÅ£Êú¨Â∫ó','È∑πÊµÅÊãâÈ∫µËá∫ÁÅ£Êú¨Â∫ó') \\\n",
    "                .replace('„É©„Éº„É°„É≥','ÊãâÈ∫µ').replace('„Çâ„Éº„ÇÅ„Çì','ÊãâÈ∫µ').replace('ÊüëÊ©òshin','ÊüëÊ©òshinn')  \\\n",
    "                .replace('È∫µÂ±ãÂ£π‰πãÁ©¥','È∫µÂ±ãÂ£π‰πãÁ©¥ichi').replace('„ÅÆ','‰πã').replace('aqua2','') \\\n",
    "                .replace('È∫µÈã™','È∫µËàñ').replace('Âè∞Êπæ','Ëá∫ÁÅ£').replace('Áï™ËåÑ','ËïÉËåÑ')  \\\n",
    "                .replace('Âè∞','Ëá∫').replace('È∫µ„ÇÑÈùíÈà¥','È∫µÂ±ãÈùíÈà¥')\n",
    "    stem_store.append(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dealing with Datetime (string -> datetime)\n",
    "ramen_date_final = []\n",
    "for date in ramen_date_raw:\n",
    "    date = date[2:]\n",
    "    date = date.replace('Âπ¥','/').replace('Êúà','/').replace('Êó•','')\n",
    "    date_obj = datetime.strptime(date, '%y/%m/%d')\n",
    "    ramen_date_final.append(date_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuchiacheng/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  if __name__ == '__main__':\n",
      "/Users/yuchiacheng/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#### Output\n",
    "df_output = pd.DataFrame(list(zip(*[ramen_shop_list_final, stem_store, ramen_date_final, ramen_name_list_final, review_lst])))\n",
    "col_names = ['stores', 'stem_store', 'create_on', 'ramen_name', 'fb_review']\n",
    "df_output.columns = col_names\n",
    "df_output[\"stores_len\"] = df_output[\"stores\"].str.len()\n",
    "df_output[\"ramen_name_len\"] = df_output[\"ramen_name\"].str.len() \n",
    "df_output[\"fb_review_len\"] = df_output[\"fb_review\"].str.len() \n",
    "df_output_ = df_output[(df_output['stores_len'] > 1.0)]\n",
    "df_output_ = df_output_[(df_output['ramen_name_len'] > 1.0)]\n",
    "df_output_ = df_output_[(df_output['fb_review_len'] > 2.0)]\n",
    "\n",
    "df_new = df_output_.sort_values(by=['stores', 'ramen_name_len'])\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "df_new = df_new.drop(columns=['stores_len','ramen_name_len','fb_review_len'])\n",
    "df_new = df_new.sort_index(axis=0 ,ascending=True)\n",
    "df_new = df_new.iloc[::-1]\n",
    "df_new = df_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping store_name with Map (create store_id) -> TABLE3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_input = list(df_new['stem_store'])\n",
    "shop_sorted = sorted(list(set(stores_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First GROUPING\n",
    "util_func = lambda x: x[0]  \n",
    "first_sort = [list(ele) for i, ele in groupby(shop_sorted, util_func)] \n",
    "lst = list(itertools.chain(*first_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SECOND GROUPING\n",
    "unique = []\n",
    "second_sort = []\n",
    "\n",
    "util_func = lambda x: x[1]\n",
    "for item in first_sort:\n",
    "    if len(item) > 1:\n",
    "        temp = sorted(item, key = util_func)\n",
    "        second_sort.append([list(ele) for i, ele in groupby(temp, util_func)])\n",
    "    else:\n",
    "        unique.append(item[0])\n",
    "\n",
    "second_sort_len = []\n",
    "\n",
    "for item_1 in second_sort:\n",
    "    for item_2 in item_1: \n",
    "        if len(item_2) > 1:\n",
    "            item_2 = sorted(item_2, key=len)\n",
    "            second_sort_len.append(item_2)\n",
    "        else:\n",
    "            unique.append(item_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIRD GROUPING\n",
    "third_sort = []\n",
    "\n",
    "util_func = lambda x: x[2]\n",
    "for item in second_sort_len:\n",
    "    if item[0][:2] == 'Â±±Âµê' or item[0][:3] == 'Ëµ§È∫µÂª†' or item[0][:4] == 'È∫µÂÆ∂‰∏âÂ£´' \\\n",
    "        or item[0][:4] == 'ÊãâÈ∫µ‰∫åÈÉé' or item[0][:3] == '‰∏ÄÈ¢®Â†Ç' or item[0][:4] == 'È≥•‰∫∫ÊãâÈ∫µ' or item[0][:4] == 'Â§™ÈôΩ' \\\n",
    "        or item[0][:4] == 'Á•ûÂ±±ÊãâÈ∫µ'or item[0][:4] == 'Ëµ§ÂùÇÊãâÈ∫µ' or item[0][:4] == 'ÂäõÈáèÊãâÈ∫µ' or item[0][:4]== '‰∫¨Ê≠£ÊãâÈ∫µ':\n",
    "        third_sort.append([item])   \n",
    "    else:\n",
    "        if len(item) > 1 and len(item[0]) >= 3:\n",
    "            temp = sorted(item, key = util_func)\n",
    "            third_sort.append([list(ele) for i, ele in groupby(temp, util_func)])\n",
    "        elif len(item) > 1 and len(item[0]) == 2:\n",
    "            third_sort.append([item])\n",
    "        else:\n",
    "            unique.append(item[0])\n",
    "        \n",
    "third_sort_len = []\n",
    "\n",
    "for item_1 in third_sort:\n",
    "    for item_2 in item_1: \n",
    "        if len(item_2) > 1:\n",
    "            item_2 = sorted(item_2, key=len)\n",
    "            third_sort_len.append(item_2)\n",
    "        else:\n",
    "            unique.append(item_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOURTH GROUPING\n",
    "fourth_sort = []\n",
    "\n",
    "util_func = lambda x: x[3]\n",
    "for item in third_sort_len:\n",
    "    if item[0][:2] == 'Â±±Âµê' or item[0][:3] == 'Ëµ§È∫µÂª†' or item[0][:4] == 'È∫µÂÆ∂‰∏âÂ£´' \\\n",
    "        or item[0][:4] == 'ÊãâÈ∫µ‰∫åÈÉé' or item[0][:3] == '‰∏ÄÈ¢®Â†Ç' or item[0][:4] == 'È≥•‰∫∫ÊãâÈ∫µ' or item[0][:4] == 'Â§™ÈôΩ' \\\n",
    "        or item[0][:4] == 'Á•ûÂ±±ÊãâÈ∫µ'or item[0][:4] == 'Ëµ§ÂùÇÊãâÈ∫µ' or item[0][:4] == 'ÂäõÈáèÊãâÈ∫µ' or item[0][:4]== '‰∫¨Ê≠£ÊãâÈ∫µ':\n",
    "        fourth_sort.append([item])   \n",
    "    else:\n",
    "        if len(item) > 1 and len(item[0]) >= 4:\n",
    "            temp = sorted(item, key = util_func)\n",
    "            fourth_sort.append([list(ele) for i, ele in groupby(temp, util_func)])        \n",
    "        elif len(item) > 1 and len(item[0]) <= 3:\n",
    "            fourth_sort.append([item])\n",
    "        else:\n",
    "            unique.append(item[0])\n",
    "\n",
    "fourth_sort_len = []\n",
    "\n",
    "for item_1 in fourth_sort:\n",
    "    for item_2 in item_1: \n",
    "        if len(item_2) > 1:\n",
    "            item_2 = sorted(item_2, key=len)\n",
    "            fourth_sort_len.append(item_2)\n",
    "        else:\n",
    "            unique.append(item_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Double Check\n",
    "done = []\n",
    "unclassified_group = []\n",
    "\n",
    "for item in fourth_sort_len:\n",
    "    grouped = []\n",
    "    unclassified = []\n",
    "    grouped.append(item[0])\n",
    "    for i in range(len(item)-1):\n",
    "        if (item[0] in item[i+1]) or (item[0][:6] in item[i+1]):\n",
    "            grouped.append(item[i+1])\n",
    "        else:\n",
    "            unclassified.append(item[i+1])\n",
    "    done.append(grouped)\n",
    "    if unclassified != []:\n",
    "        unclassified_group.append(unclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FINAL GROUPING\n",
    "for item in unclassified_group:\n",
    "    if len(item) != 1:\n",
    "        temp_grouped = []\n",
    "        temp_unclassified = []\n",
    "        temp_grouped.append(item[0])\n",
    "        for i in range(len(item)-1):\n",
    "            if (item[0] in item[i+1]) or (item[0][:len(item[0])//2] in item[i+1]):\n",
    "                temp_grouped.append(item[i+1])\n",
    "            else:\n",
    "                temp_unclassified.append(item[i+1])\n",
    "        done.append(temp_grouped)\n",
    "        #print(temp_unclassified)\n",
    "        if temp_unclassified != [] and len(temp_unclassified) == 1:\n",
    "            unique.append(temp_unclassified[0])\n",
    "    else:\n",
    "        unique.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DICTIONARY OF STORE NAME\n",
    "dict_done = {}\n",
    "for i in range(len(done)):\n",
    "    dict_done[i] = done[i]\n",
    "    \n",
    "unique_ = []\n",
    "for u in unique:\n",
    "    unique_.append([u])\n",
    "dict_unique = {i: unique_[j] for i,j in zip([ i for i in range(len(dict_done), len(unique)+len(dict_done))], [j for j in range(len(unique))])}\n",
    "\n",
    "dict_fb = dict(dict_done)\n",
    "dict_fb.update(dict_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAPPING SAME STORE NAME WITH MAP\n",
    "store_id = []\n",
    "for store in list(df_new['stem_store']):\n",
    "    for key, v in dict_fb.items():\n",
    "        if store in v:\n",
    "            store_id.append(key)\n",
    "            break\n",
    "        elif (key == list(dict_fb.keys())[-1]):\n",
    "            store_id.append('9999')\n",
    "df_new['store_id'] = store_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load map.csv\n",
    "df_table_1 = read_csv('Main_Store.csv', encoding='utf8',index_col=0)\n",
    "map_id = list(df_table_1.index)\n",
    "map_name = list(df_table_1['main_store'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Matching\n",
    "fb_match_id = []\n",
    "\n",
    "for fb_key, fb_v in dict_fb.items():\n",
    "    fb_v[0] = fb_v[0].lower()\n",
    "    fb_v[0] = fb_v[0].replace('„ÅÆ','‰πã')\n",
    "    for m_id, m_name in zip(map_id, map_name):\n",
    "        m_name = m_name.lower()\n",
    "        m_name = m_name.replace('„ÅÆ','‰πã')\n",
    "        if len(fb_v) == 1 and (fb_v[0] == m_name):\n",
    "            fb_match_id.append(m_id)\n",
    "            break\n",
    "        elif (fb_v[0] == m_name):\n",
    "            fb_match_id.append(m_id)\n",
    "            break\n",
    "        elif (len(fb_v) >= 1) and (fb_v[0] in m_name):\n",
    "            fb_match_id.append(m_id)\n",
    "            break\n",
    "        elif (len(fb_v[0]) >= 6) and (fb_v[0][:5] == m_name[:5]):\n",
    "            fb_match_id.append(m_id)\n",
    "            #print(m_name[:6])\n",
    "            break\n",
    "        elif (len(fb_v[0]) >= 5) and (len(fb_v[0]) <= 7) and (fb_v[0][:4] == m_name[:4]):\n",
    "            fb_match_id.append(m_id)\n",
    "            #print(m_name)\n",
    "            break       \n",
    "        elif (m_id == 335):\n",
    "            fb_match_id.append('99999')\n",
    "            break           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE STORE-ID\n",
    "'''\n",
    "IF the store exsits in Table_1 (Main_Store), store_id matches together;\n",
    "Otherwise, the store_id is 10000+number, which means this store only exists in FB's Post.\n",
    "'''\n",
    "table_foreign_key = []\n",
    "\n",
    "for store in list(df_new['stem_store']):\n",
    "    for (fb_k, fb_v), new in zip(dict_fb.items(), fb_match_id):\n",
    "        if store in fb_v and (new != '99999'):\n",
    "            table_foreign_key.append(new)\n",
    "            break\n",
    "        elif (store in fb_v) and (new == '99999'):\n",
    "            table_foreign_key.append(fb_k + 10000)\n",
    "            break\n",
    "        elif (fb_k == list(dict_fb.keys())[-1]) and (store not in fb_v):\n",
    "            table_foreign_key.append('99999')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create post_id \n",
    "post_id = []\n",
    "for i in range(1,len(table_foreign_key)+1):\n",
    "    if len(str(i)) == 1:\n",
    "        id_ = 'T3' + '0000' + str(i)\n",
    "    elif len(str(i)) == 2:\n",
    "        id_ = 'T3' + '000' + str(i)\n",
    "    elif len(str(i)) == 3:\n",
    "        id_ = 'T3' + '00' + str(i)\n",
    "    elif len(str(i)) == 4:\n",
    "        id_ = 'T3' + '0' + str(i)\n",
    "    elif len(str(i)) == 6:\n",
    "        id_ = 'T3' + str(i)\n",
    "    post_id.append(id_)   \n",
    "df_new['post_id'] = post_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['store_id'] = table_foreign_key \n",
    "df_t3 = df_new[['post_id','store_id','stores', 'create_on', 'ramen_name', 'fb_review']]\n",
    "#df_t3.to_csv('Post.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbasecondaa34aaf43f98b497db7a5252c786d5680"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}